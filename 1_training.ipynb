{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CS5242 Final Project : Model Training Notebook\n",
    "===\n",
    "> Transfer learning and fine-tuning pre-trained models on ImageNet dataset\n",
    "\n",
    "*Murat Shagirov*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# for plotting figures (report)\n",
    "import matplotlib\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (15,5) # use larger for presentation\n",
    "matplotlib.rcParams['font.size']= 9 # use 14 for presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "dtype: torch.float32\n",
      "Training dataset: 931 samples. \n",
      "Validation dataset: 233 samples.\n"
     ]
    }
   ],
   "source": [
    "from nn import train_model # model training function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from datautils import LoadTrainingData\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models, utils, transforms as T\n",
    "\n",
    "from datautils import BatchUnnorm, Unnorm\n",
    "\n",
    "# check for CUDA device and set default dtype\n",
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "dtype = torch.float32\n",
    "print(f'device: {device}\\ndtype: {dtype}')\n",
    "\n",
    "# Transforms\n",
    "unnorm = Unnorm() # unnormalize a single RGB image\n",
    "unnormb = BatchUnnorm() # unnormalize batch of images\n",
    "\n",
    "toPIL = T.ToPILImage()\n",
    "\n",
    "transform = T.Compose([T.ToPILImage(),\n",
    "                       T.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
    "                       T.RandomHorizontalFlip(),\n",
    "#                        T.Resize(224),\n",
    "                       T.ToTensor(),\n",
    "                       T.ConvertImageDtype(dtype), \n",
    "                       T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "val_transform = T.Compose([T.ToPILImage(),\n",
    "                           T.Resize(224),\n",
    "                           T.ToTensor(),\n",
    "                           T.ConvertImageDtype(dtype),\n",
    "                           T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "\n",
    "# Paths to training dataset and labels (before Train/Val split)\n",
    "train_csv = path.join('./datasets','train_label.csv')\n",
    "train_data_path = path.join('./datasets','train_image','train_image')\n",
    "\n",
    "np.random.seed(42) #seed np RNG for consistency\n",
    "# split the original training data into 85% / 15% train/val datasets\n",
    "datasets = LoadTrainingData(train_csv, train_data_path, transform=transform,\n",
    "                            split=True, train_percent=80, val_transform=val_transform)\n",
    "\n",
    "print(f\"Training dataset: {len(datasets['train'])} samples.\",\n",
    "      f\"\\nValidation dataset: {len(datasets['val'])} samples.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Set **batch size** and construct data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch sizes\n",
    "bsize_train = 16\n",
    "bsize_val = 16\n",
    "\n",
    "# Prepare dataloaders\n",
    "data_loaders = {'train' : DataLoader(datasets['train'], batch_size=bsize_train, shuffle=True, num_workers=0),\n",
    "                'val'   : DataLoader(datasets['val'],  batch_size=bsize_val, shuffle=False, num_workers=0)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download ImageNet pre-trained model from torchhub\n",
    "model_ft = models.resnet18(pretrained=True,progress=False)\n",
    "\n",
    "# # for transfer learning freeze (disable grads for early layers)\n",
    "# for param in model_ft.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "\n",
    "# size of each output sample: nn.Linear(num_ftrs, len(class_names)).\n",
    "model_ft.fc = nn.Linear(num_ftrs, 3)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = None # lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param_tensor in model_ft.state_dict():\n",
    "#     print(param_tensor, \"\\t\", model_ft.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19 --- train Loss: 0.6729 Acc: 0.6950 || val Loss: 0.3785 Acc: 0.8455 || 26s\n",
      "Epoch 1/19 --- train Loss: 0.3261 Acc: 0.8711 || val Loss: 0.2429 Acc: 0.9013 || 52s\n",
      "Epoch 2/19 --- train Loss: 0.2999 Acc: 0.8765 || val Loss: 0.2625 Acc: 0.8712 || 78s\n",
      "Epoch 3/19 --- train Loss: 0.2500 Acc: 0.9033 || val Loss: 0.2210 Acc: 0.9099 || 104s\n",
      "Epoch 4/19 --- train Loss: 0.1997 Acc: 0.9216 || val Loss: 0.2107 Acc: 0.9099 || 131s\n",
      "Epoch 5/19 --- train Loss: 0.1521 Acc: 0.9366 || val Loss: 0.1885 Acc: 0.9442 || 157s\n",
      "Epoch 6/19 --- "
     ]
    }
   ],
   "source": [
    "best_model, curve_data  = train_model(model_ft, optimizer_ft, data_loaders, num_epochs=20,\n",
    "                         loss_func=criterion, scheduler=exp_lr_scheduler, device=device, return_best=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[20,8])\n",
    "t = np.arange(curve_data['total_epochs'])\n",
    "plt.subplot(121)\n",
    "plt.plot(t,curve_data['trainLosses'],label='Train')\n",
    "plt.plot(t,curve_data['valLosses'],label='Val')\n",
    "plt.title('Loss'); plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(t,curve_data['trainAccs'],label='Train')\n",
    "plt.plot(t,curve_data['valAccs'],label='Val')\n",
    "plt.title('Accuracy'); plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
